{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from socket import error as SocketError\n",
    "import tweepy\n",
    "import nltk, pprint, string, csv\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "import ast, pickle, random  \n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.isdir(\"Data/Testing_data/\"):\n",
    "    os.makedirs(\"Data/Testing_data/\")\n",
    "\n",
    "def tweets_collection(curr_pair, start_date, end_date):\n",
    "    curr_pair_ = \"'\" + curr_pair + \"'\"\n",
    "    consumer_key = \"WkmvAytt1DEWva2VukcqACVtK\"\n",
    "    consumer_secret = \"jy8D7yESo1KrUc0EgIXwVWw4IUHvSy2AgQzsMIXMUb6UtM0S9p\"\n",
    "    access_token = \"447676855-FbQsPAuLttxllF8TB4eK6CV8keYZk7BEW6UCQDuw\"\n",
    "    access_secret = \"NaFfyZMKHCP9zCpm88PcoGvOh7ZQNP3kUlPaxUjS44Vxz\"\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    def removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "    \n",
    "    pairs = ['Pairs', curr_pair_]\n",
    "        \n",
    "    #read control_maximum file and get maximum id to use as a filter\n",
    "    try:\n",
    "        control = open(\"Data/Testing_data/maximum_id_\"+curr_pair+\".csv\", \"r\")\n",
    "        maxid = control.readline()\n",
    "        control.close()\n",
    "    except Exception as fileEx:\n",
    "        maxid=\"0\"\n",
    "        fileToWriteError=open(\"Errors.txt\",\"a\")\n",
    "        fileToWriteError.write(\"maxid: \" + str(fileEx) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")\n",
    "\n",
    "    twitter_search = twitter.Twitter(domain='api.twitter.com', api_version='1.1',  \n",
    "                                     auth=twitter.oauth.OAuth('447676855-FbQsPAuLttxllF8TB4eK6CV8keYZk7BEW6UCQDuw', \n",
    "                                                              'NaFfyZMKHCP9zCpm88PcoGvOh7ZQNP3kUlPaxUjS44Vxz', \n",
    "                                                              'WkmvAytt1DEWva2VukcqACVtK', \n",
    "                                                              'jy8D7yESo1KrUc0EgIXwVWw4IUHvSy2AgQzsMIXMUb6UtM0S9p'))\n",
    "\n",
    "    #hold the query results\n",
    "    tweets_ids = []\n",
    "    tweets = []\n",
    "    timestamp = []\n",
    "    tweets_users = []\n",
    "\n",
    "    #run twitter search api\n",
    "    try:    \n",
    "        for tweet in tweepy.Cursor(api.search,\n",
    "                               q=curr_pair_,\n",
    "                               since=start_date,\n",
    "                               until=end_date,\n",
    "                               count = 100,\n",
    "                               lang=\"en\").items():\n",
    "            tweets_ids.append(tweet.id), \n",
    "            tweets.append(tweet.text),\n",
    "            timestamp.append(str(tweet.created_at)),\n",
    "            tweets_users.append(tweet.user.name)\n",
    "\n",
    "    except ValueError:\n",
    "        fileToWriteErrorConnection=open(\"Errors.txt\",\"a\")\n",
    "        fileToWriteErrorConnection.write(\"twitter_res append: \" + str(ValueError) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")\n",
    "\n",
    "    pairs = ['Pairs', curr_pair]\n",
    "\n",
    "    for counter in range(0, len(tweets)):\n",
    "        #remove hashtags, user tags and user retweets\n",
    "        tweets[counter]=re.sub(r'(?<=#)\\w+| #|(?<=@)\\w+| @ |(?<=RT)\\w+|RT',\"\", tweets[counter])\n",
    "        #convert tweet to lowecase \n",
    "        tweets[counter] = tweets[counter].lower()\n",
    "        tweets[counter]= re.sub(r'[^\\w:.$]',\" \",tweets[counter])\n",
    "        #remove links\n",
    "        tweets[counter] = re.sub(r'http\\\\w+',\"\",tweets[counter])\n",
    "        tweets[counter]= re.sub(r'http:.{15}',\"\",tweets[counter])\n",
    "        tweets[counter]= re.sub(r'http:.{10}',\"\",tweets[counter])\n",
    "\n",
    "        for pairCounter in range(1, len(pairs)):\n",
    "            currentPair = pairs[pairCounter].replace('/',\"\").replace('\\r\\n',\"\").lower()\n",
    "            if currentPair in tweets[counter]:\n",
    "                if not os.path.isdir(\"Data/Testing_data/\"+currentPair.upper()):\n",
    "                    os.makedirs(\"Data/Testing_data/\"+currentPair.upper())\n",
    "                try:\n",
    "                    fileToWrite = open(\"Data/Testing_data/\"+currentPair.upper()+\"/\"+currentPair.upper()+\"TemporaryRaw\"+str(start_date)+'.csv',\"a\")\n",
    "                except Exception as fileEx:\n",
    "                    fileToWriteError=open(\"Errors.txt\",\"a\")\n",
    "                    fileToWriteError.write(\"tweetWrite: \" + str(fileEx) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")\n",
    "                fileToWrite = open(\"Data/Testing_data/\"+currentPair.upper()+\"/\"+currentPair.upper()+\"TemporaryRaw\"+str(start_date)+'.csv',\"a\")\n",
    "                tweets_users[counter] = removeNonAscii(tweets_users[counter])\n",
    "                tweets[counter] = removeNonAscii(tweets[counter])\n",
    "                fileToWrite.write(str(tweets_ids[counter]) +\",\"+tweets_users[counter]+\",\"\n",
    "                                      +tweets[counter]+\",\"+time.strftime(timestamp[counter])+\"\\n\")\n",
    "                fileToWrite.close()\n",
    "\n",
    "    print ('Collected: ' + str(len(tweets_ids)) + ' tweet(s)')\n",
    "            \n",
    "    if len(tweets_ids) > 0:\n",
    "        controlIdFile = open(\"Data/Testing_data/maximum_id_\"+curr_pair+\".csv\",\"w\")\n",
    "        try:\n",
    "            controlIdFile.write(str(max(tweets_ids)))\n",
    "            controlIdFile.close()\n",
    "        except Exception as fileEx:\n",
    "            fileToWriteError=open(\"Errors.txt\",\"a\")\n",
    "            fileToWriteError.write(\"maxID writeback: \" + str(fileEx) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-01-07 2019-01-08 2019-01-09 2019-01-11 2019-01-12 2019-01-13 2019-01-14 2019-01-10 2019-01-15 2019-01-16 2019-01-17 2019-01-18 2019-01-19 2019-01-20 2019-01-21 2019-01-22 2019-01-23 2019-01-24 2019-01-25 2019-01-26 2019-01-27 2019-01-28 2019-01-29 2019-01-30 2019-01-31 2019-02-01 2019-02-02 2019-02-03 2019-02-04 2019-02-05 2019-02-06 2019-02-07 2019-02-08 2019-02-09 2019-02-10 2019-02-11 2019-02-12 2019-02-13 2019-02-14 2019-02-15 2019-02-16 2019-02-17 2019-02-18 2019-02-19 2019-02-20 2019-02-21 2019-02-22 2019-02-23 2019-02-24 2019-02-25 2019-02-26 2019-02-27 2019-02-28 2019-03-01 2019-03-02 2019-03-03 2019-03-04 2019-03-05 2019-03-06 2019-03-07 2019-03-08 2019-03-09 2019-03-10 2019-03-11 2019-03-12 2019-03-13 2019-03-14 2019-03-15 2019-03-16 2019-03-17 2019-03-18 2019-03-19 2019-03-20 2019-03-21 2019-03-22 2019-03-23 2019-03-24 2019-03-25 2019-03-26 2019-03-27 2019-03-28 2019-03-29 2019-03-30 2019-03-31 2019-04-01 2019-04-02 2019-04-03 2019-04-04 2019-04-05 2019-04-06 2019-04-07 2019-04-08 2019-04-09 2019-04-10 2019-04-11 2019-04-12 2019-04-13 2019-04-14 2019-04-15 2019-04-16 2019-04-17 2019-04-18 2019-04-19 2019-04-20 2019-04-21 2019-04-22 2019-04-23 2019-04-24 2019-04-25 2019-04-26 2019-04-27 2019-04-28 2019-04-29 2019-04-30 2019-05-01 2019-05-02 2019-05-03 2019-05-04 2019-05-05 2019-05-06 2019-05-07 2019-05-08 2019-05-09 2019-05-10 2019-05-11 2019-05-12 2019-05-13 2019-05-14 2019-05-15 2019-05-16 2019-05-17 2019-05-18 2019-05-19 2019-05-20 2019-05-21 2019-05-22 2019-05-23 2019-05-24 2019-05-25 2019-05-26 2019-05-28 2019-05-29 2019-05-30 2019-05-31 2019-06-01 2019-06-02 2019-06-03 2019-06-04 ']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b7c0eaca19458b860e2f83b2e1cda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Tweet collection', max=150, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run tweets collection for 2019-05-27 EURUSD\n",
      "Collected: 1068 tweet(s)\n",
      "Run tweets collection for 2019-05-27 GBPUSD\n",
      "Collected: 764 tweet(s)\n",
      "Run tweets collection for 2019-05-27 USDJPY\n",
      "Collected: 478 tweet(s)\n",
      "Run tweets collection for 2019-05-27 USDCAD\n",
      "Collected: 318 tweet(s)\n",
      "Run tweets collection for 2019-05-27 USDCHF\n",
      "Collected: 245 tweet(s)\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "import datetime\n",
    "import os.path\n",
    "from tqdm import tqdm, tnrange\n",
    "\n",
    "curr_list = ['EURUSD', 'GBPUSD', 'USDJPY', 'USDCAD', 'USDCHF']\n",
    "\n",
    "# First day in database\n",
    "date0 = date(2019, 1, 7) # year, month, day\n",
    "\n",
    "# Today\n",
    "today = datetime.date.today()\n",
    "\n",
    "# Open txt file to write\n",
    "text_file = open(\"Data/Database_content.txt\", \"r\")\n",
    "lines = text_file.readlines()\n",
    "print(lines)\n",
    "text_file.close()\n",
    "\n",
    "for i in tnrange(150, desc='Tweet collection'):\n",
    "    lower_bound = date0 + timedelta(days=i)\n",
    "    upper_bound = lower_bound + timedelta(days=1)\n",
    "    \n",
    "    if today > lower_bound:\n",
    "        if str(lower_bound) not in str(lines):\n",
    "#             print('No such date --> work further to get it')\n",
    "            text_file = open(\"Data/Database_content.txt\", \"a\")\n",
    "            start_date = lower_bound\n",
    "            end_date = upper_bound\n",
    "            \n",
    "            text_file.write(str(lower_bound))\n",
    "            text_file.write(' ')\n",
    "            for c in curr_list:\n",
    "                print('Run tweets collection for {} {}'.format(start_date, c))\n",
    "                curr_pair = c\n",
    "#                 print(curr_pair, start_date, end_date)\n",
    "                tweets_collection(curr_pair, start_date, end_date)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
